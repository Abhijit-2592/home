$ python -m spacy train -T -P en output_dir  train.json  train.json
dropout_from = 0.2 by default
dropout_to = 0.2 by default
dropout_decay = 0.0 by default
batch_from = 1 by default
batch_to = 16 by default
batch_compound = 1.001 by default
max_doc_len = 5000 by default
beam_width = 1 by default
beam_density = 0.0 by default
Warning: Unnamed vectors -- this won't allow multiple vectors models to be loaded. (Shape: (0, 0))
learn_rate = 0.001 by default
optimizer_B1 = 0.9 by default
optimizer_B2 = 0.999 by default
optimizer_eps = 1e-08 by default
L2_penalty = 1e-06 by default
grad_norm_clip = 1.0 by default
parser_hidden_depth = 1 by default
parser_maxout_pieces = 2 by default
token_vector_width = 128 by default
hidden_width = 200 by default
embed_size = 7000 by default
history_feats = 0 by default
history_width = 0 by default
Itn.    P.Loss  N.Loss  UAS     NER P.  NER R.  NER F.  Tag %   Token %
0       0.000   31.226  16.901  28.571  50.000  36.364  0.000   100.000 5636.5  0.0
1       0.000   10.506  16.901  75.000  75.000  75.000  0.000   100.000 5960.4  0.0
2       0.000   5.312   16.901  100.000 100.000 100.000 0.000   100.000 6142.3  0.0
3       0.000   0.630   16.901  100.000 100.000 100.000 0.000   100.000 6082.1  0.0
4       0.000   0.000   16.901  100.000 100.000 100.000 0.000   100.000 6206.9  0.0
5       0.000   0.000   16.901  100.000 100.000 100.000 0.000   100.000 6016.0  0.0
6       0.000   0.000   16.901  100.000 100.000 100.000 0.000   100.000 6214.6  0.0
7       0.000   0.000   16.901  100.000 100.000 100.000 0.000   100.000 5838.5  0.0
8       0.000   0.290   16.901  100.000 100.000 100.000 0.000   100.000 6126.5  0.0
9       0.000   0.000   16.901  100.000 100.000 100.000 0.000   100.000 6109.0  0.0
10      0.000   0.000   16.901  100.000 100.000 100.000 0.000   100.000 6115.1  0.0
11      0.000   0.401   16.901  100.000 100.000 100.000 0.000   100.000 6089.2  0.0
12      0.000   0.000   16.901  100.000 100.000 100.000 0.000   100.000 5835.6  0.0
13      0.000   0.016   16.901  100.000 100.000 100.000 0.000   100.000 6190.9  0.0
14      0.000   0.000   16.901  100.000 100.000 100.000 0.000   100.000 6049.2  0.0
15      0.000   1.950   16.901  100.000 100.000 100.000 0.000   100.000 6088.3  0.0
16      0.000   0.010   16.901  100.000 100.000 100.000 0.000   100.000 5753.5  0.0
17      0.000   0.000   16.901  100.000 100.000 100.000 0.000   100.000 6032.1  0.0
18      0.000   0.000   16.901  100.000 100.000 100.000 0.000   100.000 6072.8  0.0
19      0.000   0.000   16.901  100.000 100.000 100.000 0.000   100.000 6109.0  0.0
20      0.000   0.000   16.901  100.000 100.000 100.000 0.000   100.000 6060.6  0.0
21      0.000   0.024   16.901  100.000 100.000 100.000 0.000   100.000 6008.1  0.0
22      0.000   2.000   16.901  100.000 100.000 100.000 0.000   100.000 6049.5  0.0
23      0.000   0.000   16.901  100.000 100.000 100.000 0.000   100.000 6083.1  0.0
24      0.000   0.000   16.901  100.000 100.000 100.000 0.000   100.000 5873.7  0.0
25      0.000   0.000   16.901  100.000 100.000 100.000 0.000   100.000 6062.8  0.0
26      0.000   0.000   16.901  100.000 100.000 100.000 0.000   100.000 5973.1  0.0
27      0.000   0.000   16.901  100.000 100.000 100.000 0.000   100.000 5817.8  0.0
28      0.000   0.000   16.901  100.000 100.000 100.000 0.000   100.000 6085.3  0.0
29      0.000   0.000   16.901  100.000 100.000 100.000 0.000   100.000 6027.0  0.0
Saving model...
